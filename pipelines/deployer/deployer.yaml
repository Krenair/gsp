groups:
- name: deploy
  jobs:
  - update
  - deploy
- name: destroy
  jobs:
  - destroy

terraform_source: &terraform_source
  env_name: ((account-name))
  backend_type: s3
  backend_config: &terraform_backend_config
    bucket: cd-gsp-private-qndvvc
    region: eu-west-2
  vars:
    account_id: ((account-id))
    account_name: ((account-name))
    cluster_name: ((cluster-name))
    cluster_domain: ((cluster-domain))
    cluster_number: ((cluster-number))
    aws_account_role_arn: ((account-role-arn))
    github_client_id: ((github-client-id))
    github_client_secret: ((github-client-secret))
    splunk_enabled: ((splunk-enabled))
    splunk_hec_url: ((splunk-hec-url))
    k8s_splunk_hec_token: ((k8s-splunk-hec-token))
    k8s_splunk_index: ((k8s-splunk-index))
    hsm_splunk_hec_token: ((hsm-splunk-hec-token))
    hsm_splunk_index: ((hsm-splunk-index))
    vpc_flow_log_splunk_hec_token: ((vpc-flow-log-splunk-hec-token))
    vpc_flow_log_splunk_index: ((vpc-flow-log-splunk-index))
    eks_version: ((eks-version))
    worker_instance_type: ((worker-instance-type))
    worker_count: ((worker-count))
    ci_worker_instance_type: ((ci-worker-instance-type))
    ci_worker_count: ((ci-worker-count))

task_image_resource: &task_image_resource
  type: docker-image
  source: {repository: "govsvc/task-toolbox", tag: "1.3.0"}

generate_cluster_values: &generate_cluster_values
  platform: linux
  image_resource: *task_image_resource
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      mkdir -p cluster-values
      echo "fetching cluster-values file from cluster-state..."
      jq -r '.values' ./cluster-state/metadata > ./cluster-values/values.yaml
      cat ./cluster-values/values.yaml
  inputs:
  - name: cluster-state
  outputs:
  - name: cluster-values

generate_user_values: &generate_user_values
  platform: linux
  image_resource: *task_image_resource
  params:
    ACCOUNT_ID: ((account-id))
    CLUSTER_NAME: ((cluster-name))
    PATH_TO_USERS: ((users-path))
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      cd users
      echo "creating helm compatible values file from user data"
      yq . ${PATH_TO_USERS}/*.yaml \
        | jq ". + {roleARN: (\"arn:aws:iam::${ACCOUNT_ID}:role/${CLUSTER_NAME}-user-\" + .name)}" \
        | jq -s '{users: .}' \
        | yq --yaml-output .\
        > ../user-values/values.yaml
      cat ../user-values/values.yaml
  inputs:
  - name: users
  outputs:
  - name: user-values

generate_users_terraform: &generate_users_terraform
  platform: linux
  image_resource: *task_image_resource
  params:
    PATH_TO_USERS: ((users-path))
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      mkdir -p users-terraform
      cd users
      echo "creating terraform for user roles from user data"
      yq . ${PATH_TO_USERS}/*.yaml \
        | jq '[{key: (.name | gsub("[^a-z-A-Z0-9]"; "-")), value: {source: "../platform/modules/gsp-user", user_name: .name, user_arn: .ARN, cluster_name: "${var.cluster_name}"}}] | from_entries' \
        | jq -s '{module: . | add, variable: { aws_account_role_arn: { type: "string" }, cluster_name: { type: "string" } }, provider: { aws: { region: "eu-west-2", assume_role: { role_arn: "${var.aws_account_role_arn}" } } } }' \
        > ../users-terraform/users.tf.json
      cat ../users-terraform/users.tf.json
  inputs:
  - name: users
  outputs:
  - name: users-terraform

generate_namespace_values: &generate_namespace_values
  platform: linux
  image_resource: *task_image_resource
  params:
    PATH_TO_NAMESPACES: ((config-path))/namespaces/
    CLUSTER_NAME: ((cluster-name))
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      echo "creating tmp dirs"
      mkdir -p namespace-values
      namespace_values_file="$(pwd)/namespace-values/values.yaml"
      mkdir ns-chart
      cd config/${PATH_TO_NAMESPACES}
      echo "creating helm compatible values file from namespace data"
      echo "--> ${CLUSTER_NAME}-main"
      echo 'namespaces:' > $namespace_values_file
      echo "- name: ${CLUSTER_NAME}-main" >> $namespace_values_file
      echo "  resources: []" >> $namespace_values_file
      for ns in $(ls | grep -E "^${CLUSTER_NAME}-"); do
        echo "--> ${ns}"
        echo "- name: ${ns}" >> $namespace_values_file
        set_namespace=".metadata.namespace=\"${ns}\""
        combined_resources="$(yq . ${ns}/*.yaml | jq $set_namespace | jq -s -c .)"
        echo "  resources: $combined_resources" >> $namespace_values_file
      done
      cat $namespace_values_file
  inputs:
  - name: config
  outputs:
  - name: namespace-values

apply_cluster_chart: &apply_cluster_chart
  platform: linux
  image_resource: *task_image_resource
  params:
    ACCOUNT_ROLE_ARN: ((account-role-arn))
    ACCOUNT_NAME: ((account-name))
    CLUSTER_NAME: ((cluster-name))
    AWS_REGION: eu-west-2
    AWS_DEFAULT_REGION: eu-west-2
    CHART_NAME: gsp-cluster
    DEFAULT_NAMESPACE: gsp-system
    CHART_RELEASE_NAME: gsp
    GITHUB_API_TOKEN: ((github-api-token))
    CLUSTER_PRIVATE_KEY: ((ci-system-gpg-private))
    CLUSTER_PUBLIC_KEY: ((ci-system-gpg-public))
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      echo "assuming aws deployer role..."
      AWS_CREDS="$(aws-assume-role $ACCOUNT_ROLE_ARN)"
      eval "${AWS_CREDS}"
      echo "fetching kubeconfig from aws..."
      aws eks update-kubeconfig \
        --name "${CLUSTER_NAME}" \
        --kubeconfig ./kubeconfig
      export KUBECONFIG=$(pwd)/kubeconfig
      echo "setting default namespace to ${DEFAULT_NAMESPACE}"
      kubectl config set-context $(kubectl config get-contexts -o name) \
        --namespace "${DEFAULT_NAMESPACE}"
      echo "rendering ${CHART_NAME} chart..."
      mkdir -p manifests
      helm template \
        --name "${CHART_RELEASE_NAME}" \
        --namespace "${DEFAULT_NAMESPACE}" \
        --values cluster-values/values.yaml \
        --values user-values/values.yaml \
        --values namespace-values/values.yaml \
        --set githubAPIToken=${GITHUB_API_TOKEN} \
        --set "global.cluster.privateKey=${CLUSTER_PRIVATE_KEY}" \
        --set "global.cluster.publicKey=${CLUSTER_PUBLIC_KEY}" \
        --output-dir manifests \
        "platform/charts/${CHART_NAME}"
      echo "rendering gsp-istio chart..."
      helm template \
        --name istio \
        --namespace istio-system \
        --output-dir manifests \
        --set global.runningOnAws=true \
        platform/charts/gsp-istio
      function apply() {
        echo "applying ${1} from ${CHART_NAME} chart..."
        until kubectl apply -R -f $1; do
          echo "---> ${1} apply failed retrying in 5s..."
          sleep 5
        done
        sleep 5 # FIXME: we should do something smarter than sleep and check for success
        echo "---> ${1} applied OK!"
      }
      apply manifests/${CHART_NAME}/templates/00-aws-auth/
      apply manifests/gsp-istio
      apply manifests/${CHART_NAME}/templates/01-aws-system/
      apply manifests/
  inputs:
  - name: cluster-values
  - name: user-values
  - name: namespace-values
  - name: platform

run_conformance_tests: &run_conformance_tests
  platform: linux
  image_resource: *task_image_resource
  params:
    ACCOUNT_ROLE_ARN: ((account-role-arn))
    ACCOUNT_NAME: ((account-name))
    CLUSTER_NAME: ((cluster-name))
    DEFAULT_NAMESPACE: gsp-system
    AWS_REGION: eu-west-2
    AWS_DEFAULT_REGION: eu-west-2
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      echo "assuming aws deployer role..."
      AWS_CREDS="$(aws-assume-role $ACCOUNT_ROLE_ARN)"
      eval "${AWS_CREDS}"
      echo "fetching kubeconfig from aws..."
      aws eks update-kubeconfig \
        --name "${CLUSTER_NAME}" \
        --kubeconfig ./kubeconfig
      export KUBECONFIG=$(pwd)/kubeconfig
      echo "setting default namespace to ${DEFAULT_NAMESPACE}"
      kubectl config set-context $(kubectl config get-contexts -o name) \
        --namespace "${DEFAULT_NAMESPACE}"

      echo "beginning conformance test..."
      mkdir -p plugins/e2e/results

      function cleanup() {
        echo "cleaning up sonobuoy..."
         sonobuoy delete --wait
      }
      trap 'cleanup' INT TERM EXIT

      sonobuoy run \
        --wait \
        --sonobuoy-image "gcr.io/heptio-images/sonobuoy:v0.14.2" \
        --plugin e2e \
        --e2e-focus "Pods should be submitted and removed" \
        --kube-conformance-image "govsvc/conformance-amd64:0.0.1559644071" \
        --plugin-env e2e.ALLOWED_NOT_READY_NODES=$(kubectl get nodes --selector "! node-role.kubernetes.io/worker"  --no-headers=true | wc -l) # only wait for worker nodes

      sleep 10 # wait for results to be written
      results=$(sonobuoy retrieve)
      sonobuoy e2e ${results}
      passed=$(sonobuoy e2e ${results} --show passed | head -n1)
      failed=$(sonobuoy e2e ${results} --show failed | head -n1)

      if [[ ${passed} == "passed tests: 1" && ${failed} == "failed tests: 0" ]]; then
        echo "SUCCESS"
        exit 0
      fi

      echo "FAIL"
      exit 1
  inputs:
  - name: cluster-values
  - name: user-values
  - name: namespace-values
  - name: platform

run_cloudwatch_test: &run_cloudwatch_test
    platform: linux
    image_resource: *task_image_resource
    params:
      ACCOUNT_ROLE_ARN: ((account-role-arn))
      ACCOUNT_NAME: ((account-name))
      CLUSTER_NAME: ((cluster-name))
      DEFAULT_NAMESPACE: gsp-system
      AWS_REGION: eu-west-2
      AWS_DEFAULT_REGION: eu-west-2
    run:
      path: /bin/bash
      args:
      - -eu
      - -c
      - |
        echo "assuming aws deployer role..."
        AWS_CREDS="$(aws-assume-role $ACCOUNT_ROLE_ARN)"
        eval "${AWS_CREDS}"
        echo "fetching kubeconfig from aws..."
        aws eks update-kubeconfig \
          --name "${CLUSTER_NAME}" \
          --kubeconfig ./kubeconfig
        export KUBECONFIG=$(pwd)/kubeconfig
        echo "setting default namespace to ${DEFAULT_NAMESPACE}"
        kubectl config set-context $(kubectl config get-contexts -o name) \
          --namespace "${DEFAULT_NAMESPACE}"
        echo "beginning cloudwatch test..."
        mkdir -p plugins/e2e/results
        function cleanup() {
          echo "cleaning up sonobuoy..."
          sonobuoy delete --wait
        }
        trap 'cleanup' INT TERM EXIT
        sonobuoy run \
          --wait \
          --sonobuoy-image "gcr.io/heptio-images/sonobuoy:v0.14.2" \
          --plugin cloudwatch-test \
          --plugin-env cloudwatch-test.TEST_LOGS_SINCE=$(date '+%s') \
          --kube-conformance-image "govsvc/conformance-amd64:0.0.1559644071"
        sonoexitcode=$?
        echo "Sonobuoys exit code: $sonoexitcode"
        sleep 10 # wait for results to be written
        results=$(sonobuoy retrieve) && \
          mkdir results && \
          tar xzvf $results -C results
        cat results/plugins/cloudwatch-test/results/*/output
        passed=$(grep "^PASS:" results/plugins/cloudwatch-test/results/something)
        if [[ ${passed} == "PASS: Logs have been reached cloudwatch" ]]; then
          echo "SUCCESS"
          exit 0
        fi
        echo "FAIL"
        exit 1
    inputs:
    - name: cluster-values
    - name: user-values
    - name: namespace-values
    - name: platform

drain_cluster_task: &drain_cluster_task
  platform: linux
  image_resource: *task_image_resource
  params:
    ACCOUNT_ROLE_ARN: ((account-role-arn))
    AWS_REGION: eu-west-2
    AWS_DEFAULT_REGION: eu-west-2
    CLUSTER_NAME: ((cluster-name))
  run:
    path: /bin/bash
    args:
    - -eu
    - -c
    - |
      echo "assuming aws deployer role..."
      AWS_CREDS="$(aws-assume-role $ACCOUNT_ROLE_ARN)"
      eval "${AWS_CREDS}"
      echo "checking there is a cluster to drain"
      if ! (aws eks list-clusters 1>/dev/null && aws eks describe-cluster --name "${CLUSTER_NAME}" 2>/dev/null 1>/dev/null); then
        echo 'no eks cluster running: skipping drain'
        exit 0
      fi
      echo "fetching kubeconfig from aws..."
      aws eks update-kubeconfig --name "${CLUSTER_NAME}" --kubeconfig ./kubeconfig
      export KUBECONFIG=$(pwd)/kubeconfig
      echo "fetching cluster VPC ID..."
      CLUSTER_VPC_ID=$(aws eks describe-cluster --name "${CLUSTER_NAME}" | jq .cluster.resourcesVpcConfig.vpcId -r)
      echo "deleting any LoadBalancer services..."
      kubectl get svc -o json --all-namespaces | jq '.items[] | select(.spec.type == "LoadBalancer")' | kubectl delete -f - --wait
      echo "checking for any ELBs that belong to cluster..."
      ELB_ARNS_JSON=$(aws elbv2 describe-load-balancers | jq "{LoadBalancerArns: [ .LoadBalancers[] | select(.VpcId == \"${CLUSTER_VPC_ID}\") | select(.LoadBalancerName != \"${CLUSTER_NAME}-ingress\") | .LoadBalancerArn ]}" -c)
      ELB_ARNS_COUNT=$(echo $ELB_ARNS_JSON | jq '.LoadBalancerArns | length')
      echo "waiting for ${ELB_ARNS_COUNT} ELBs to terminate..."
      if [[ "${ELB_ARNS_COUNT}" != "0" ]]; then
        aws elbv2 wait load-balancers-deleted --cli-input-json "${ELB_ARNS_JSON}"
      fi
      echo "checking for ASGs that belong to this cluster..."
      CLUSTER_ASGS=$(aws autoscaling describe-auto-scaling-groups | jq -r ".AutoScalingGroups[] | select( .Tags[].Key == \"kubernetes.io/cluster/${CLUSTER_NAME}\")" | jq -r .AutoScalingGroupName)
      for ASG_NAME in $CLUSTER_ASGS; do
        echo "scaling ${ASG_NAME} to zero..."
        aws autoscaling update-auto-scaling-group --auto-scaling-group-name "${ASG_NAME}" --min-size 0 --max-size 0 --desired-capacity 0
      done
      echo "checking if any nodes are still running ..."
      for ASG_NAME in $CLUSTER_ASGS; do
        echo "checking number of instances remaining in ${ASG_NAME}..."
        INSTANCES=$(aws autoscaling describe-auto-scaling-groups --auto-scaling-group-names "${ASG_NAME}" --query "AutoScalingGroups[0].Instances[*].InstanceId" --output text)
        if [ ! -z "$INSTANCES" ]; then
          echo "waiting for following instances to terminate in ${ASG_NAME}: ${INSTANCES}..."
          aws ec2 wait instance-terminated --instance-ids $INSTANCES
        fi
      done
  inputs:
  - name: platform

resource_types:
- name: terraform
  type: registry-image
  source:
    repository: "govsvc/terraform-resource"
    tag: "0.13.0-beta.2"
- name: github
  type: registry-image
  source:
    repository: "govsvc/concourse-github-resource"
    tag: "v0.0.2"

resources:
- name: platform
  type: github
  source:
    uri: ((platform-uri))
    organization: ((platform-organization))
    repository: ((platform-repository))
    github_api_token: ((github-api-token))
    approvers: ((github-approvers))
    required_approval_count: ((github-approval-count))
    branch: ((platform-version))
    commit_verification_keys: ((trusted-developer-keys))
- name: config
  type: github
  source:
    uri: ((config-uri))
    organization: ((config-organization))
    repository: ((config-repository))
    github_api_token: ((github-api-token))
    approvers: ((github-approvers))
    required_approval_count: ((github-approval-count))
    branch: ((config-version))
    commit_verification_keys: ((trusted-developer-keys))
- name: users
  type: github
  source:
    uri: ((users-uri))
    organization: ((users-organization))
    repository: ((users-repository))
    github_api_token: ((github-api-token))
    approvers: ((github-approvers))
    required_approval_count: ((github-approval-count))
    branch: ((users-version))
    commit_verification_keys: ((trusted-developer-keys))
    private_key: ((users-deployment-key))
- name: cluster-state
  type: terraform
  source:
    <<: *terraform_source
    backend_config:
      <<: *terraform_backend_config
      key: cluster-((cluster-name)).tfstate
- name: user-state
  type: terraform
  source:
    <<: *terraform_source
    backend_config:
      <<: *terraform_backend_config
      key: users-((cluster-name)).tfstate

jobs:
- name: update
  serial_groups: [cluster-modification]
  plan:
  - get: platform
    trigger: ((platform-trigger))
  - get: config
    trigger: ((config-trigger))
  - get: users
    trigger: ((users-trigger))
  - task: set-pipelines
    params:
      CONCOURSE_TEAM: ((concourse-team))
      CONCOURSE_USERNAME: ((concourse-username))
      CONCOURSE_PASSWORD: ((readonly_local_user_password))
      CONCOURSE_URL: ((concourse-url))
      CONCOURSE_PIPELINE_PATH: platform/pipelines/deployer/deployer.yaml
      CONCOURSE_PIPELINE_NAME: ((concourse-pipeline-name))
      CLUSTER_NAME: ((cluster-name))
      CLUSTER_CONFIG_VARS: config/((config-path))/clusters/((cluster-name)).yaml
      DEFAULT_CONFIG_VARS: platform/pipelines/deployer/deployer.defaults.yaml
      SKIP_UPDATE: ((disable-update))
    config:
      platform: linux
      image_resource: *task_image_resource
      inputs:
      - name: platform
      - name: config
      - name: users
      run:
        path: sh
        args:
        - -uec
        - |
          if [[ "${SKIP_UPDATE}" == "true" ]]; then
            echo "skipping updating pipeline as SKIP_UPDATE==true"
            exit 0
          fi
          echo "generating list of pipeline approvers..."
          echo -n "github-approvers: " > approvers.yaml
          yq . users/users/*.yaml \
            | jq -c -s "[.[] | select(.roles[] | select((. == \"${CLUSTER_NAME}-sre\" ) or (. == \"${CLUSTER_NAME}-admin\"))) | .github] | unique | sort" \
            >> approvers.yaml
          echo "generating list of trusted-developer-keys for pipeline changes"
          echo -n "trusted-developer-keys: " > trusted-developer-keys.yaml
          yq . users/users/*.yaml \
            | jq -c -s '[ .[].pub ] | sort' \
            >> trusted-developer-keys.yaml
          echo "downloading correct fly version..."
          curl -L --fail "${CONCOURSE_URL}/api/v1/cli?arch=amd64&platform=linux" > fly
          chmod +x fly
          echo "configuring fly target..."
          ./fly --target self \
            login \
            --concourse-url "${CONCOURSE_URL}" \
            --username "${CONCOURSE_TEAM}" \
            --password "${CONCOURSE_PASSWORD}" \
            --team-name "${CONCOURSE_TEAM}"
          echo "validating the pipeline..."
          ./fly --target self \
            validate-pipeline \
            --config "${CONCOURSE_PIPELINE_PATH}" \
            --load-vars-from "${DEFAULT_CONFIG_VARS}" \
            --load-vars-from "${CLUSTER_CONFIG_VARS}" \
            --load-vars-from "approvers.yaml" \
            --load-vars-from "trusted-developer-keys.yaml"
          echo "updating pipeline..."
          ./fly --target self \
            set-pipeline \
            --check-creds \
            --pipeline "${CONCOURSE_PIPELINE_NAME}" \
            --config "${CONCOURSE_PIPELINE_PATH}" \
            --load-vars-from "${DEFAULT_CONFIG_VARS}" \
            --load-vars-from "${CLUSTER_CONFIG_VARS}" \
            --load-vars-from "approvers.yaml" \
            --load-vars-from "trusted-developer-keys.yaml" \
            --non-interactive
          echo "OK!"

- name: deploy
  serial: true
  serial_groups: [cluster-modification]
  plan:
  - get: platform
    passed: ["update"]
    trigger: ((platform-trigger))
  - get: config
    passed: ["update"]
    trigger: ((config-trigger))
  - get: users
    passed: ["update"]
    trigger: ((users-trigger))
  - put: cluster-state
    params:
      env_name: ((account-name))
      terraform_source: platform/pipelines/deployer
  - aggregate:
    - task: generate-cluster-values
      timeout: 10m
      config: *generate_cluster_values
    - task: generate-namespace-values
      timeout: 10m
      config: *generate_namespace_values
    - task: generate-user-values
      timeout: 10m
      config: *generate_user_values
    - task: generate-user-terraform
      timeout: 10m
      config: *generate_users_terraform
  - put: user-state
    params:
      env_name: ((account-name))
      terraform_source: users-terraform
  - task: apply-cluster-chart
    timeout: 10m
    config: *apply_cluster_chart
    #  - task: run-conformance-tests
    #    timeout: 15m
    #    config: *run_conformance_tests
  - task: run-cloudwatch-test
    timeout: 10m
    config: *run_cloudwatch_test
- name: destroy
  serial: true
  serial_groups: [cluster-modification]
  disable_manual_trigger: ((disable-destroy))
  plan:
  - get: config
  - get: users
  - get: platform
  - task: drain-cluster
    timeout: 30m
    config: *drain_cluster_task
  - task: generate-user-terraform
    timeout: 10m
    config: *generate_users_terraform
  - put: cluster-state
    params:
      env_name: ((account-name))
      terraform_source: platform/pipelines/deployer
      action: destroy
    get_params:
      action: destroy
  - put: user-state
    params:
      env_name: ((account-name))
      terraform_source: users-terraform
      action: destroy
    get_params:
      action: destroy
